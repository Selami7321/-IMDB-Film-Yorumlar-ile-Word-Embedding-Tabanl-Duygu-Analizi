# -*- coding: utf-8 -*-
"""BYM308-ODEV4_WORD_EMBEDDING_GRUP18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/118MsDgQ3Pgcfi3Z_BNwrlVwNfCSxqzyP

Ä°STANBUL SAÄLIK VE TEKNOLOJÄ° ÃœNÄ°VERSÄ°TESÄ°

YAZILIM MÃœHENDÄ°SLÄ°ÄÄ°

ODEV4_WORD_EMBEDDING

HAZIRLAYANLAR:

SELAMÄ° Ã‡ETÄ°N 220609012

UÄUR BAKÄ° ARSLAN 220609015

YUNUS EMRE SEVÄ°NÃ‡ 220609007

Word Embedding ile NLP Problemi Ã‡Ã¶zÃ¼mÃ¼: Duygu Analizi
Bu Ã§alÄ±ÅŸmada, IMDb film incelemeleri veri setini kullanarak bir duygu analizi (sentiment analysis) problemi Ã¼zerinde word embedding yÃ¶ntemlerinin etkisini inceleyeceÄŸiz.

1. Veri Seti ve Problem TanÄ±mÄ±
Veri Seti: IMDb Movie Reviews (50,000 inceleme, pozitif/negatif etiketli)
Problem: Metin tabanlÄ± duygu analizi (binary classification)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D
from tensorflow.keras.callbacks import EarlyStopping

"""Word Embedding Kullanmadan Ã‡Ã¶zÃ¼m (Geleneksel YÃ¶ntemler)

KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±:

NumPy: SayÄ±sal hesaplamalar iÃ§in.

Pandas: Veri Ã§erÃ§eveleriyle (DataFrame) Ã§alÄ±ÅŸmak iÃ§in.

Matplotlib: Grafik Ã§izimi iÃ§in.

**Makine Ã–ÄŸrenmesi ve Derin Ã–ÄŸrenme AraÃ§larÄ±:**

train_test_split: Veriyi eÄŸitim ve test kÃ¼melerine ayÄ±rmak iÃ§in.

CountVectorizer / TfidfVectorizer: Metin verisini sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in.

MultinomialNB: Naive Bayes sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±.

LogisticRegression: Lojistik regresyon modeli.

accuracy_score / classification_report: Modelin doÄŸruluÄŸunu ve detaylÄ± performansÄ±nÄ± Ã¶lÃ§mek iÃ§in.

**Keras (Derin Ã–ÄŸrenme) AraÃ§larÄ±:**

 Tokenizer: Metinleri sayÄ±sal verilere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in (embedding Ã¶ncesi).

pad_sequences: Dizi uzunluklarÄ±nÄ± eÅŸitlemek iÃ§in.

Sequential: KatmanlarÄ± sÄ±rayla tanÄ±mlamak iÃ§in basit bir model yapÄ±sÄ±.

Embedding, LSTM vs.: Derin Ã¶ÄŸrenme modelinde kullanÄ±lacak katmanlar.

EarlyStopping: Model fazla eÄŸitildiÄŸinde (overfitting) erken durdurmak iÃ§in.

** Etiketlerin DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi:**

Metinsel etiketler (positive, negative) sayÄ±sal hale getirilir:

1 â†’ pozitif

0 â†’ negatif

**Verinin BÃ¶lÃ¼nmesi:**

X: Yorum metinleri

y: Duygu etiketleri

EÄŸitim ve test verisi %80 eÄŸitim, %20 test olarak bÃ¶lÃ¼nÃ¼r.

random_state=42: AynÄ± ÅŸekilde bÃ¶lÃ¼nmesi iÃ§in rastgelelik kontrol altÄ±na alÄ±nÄ±r (tekrarlanabilirlik saÄŸlanÄ±r).
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D
from tensorflow.keras.callbacks import EarlyStopping
try:
    df = pd.read_csv('IMDB Dataset.csv')
except:
    from tensorflow.keras.datasets import imdb
    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
    # Veriyi DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme
    df_train = pd.DataFrame({'review': train_data, 'sentiment': train_labels})
    df_test = pd.DataFrame({'review': test_data, 'sentiment': test_labels})
    df = pd.concat([df_train, df_test])
    df['review'] = df['review'].apply(lambda x: ' '.join([str(i) for i in x]))


df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})

# Veriyi bÃ¶lme
X = df['review']
y = df['sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**âœ… SonuÃ§:**

Bu kod bloÄŸu, duygu analizi iÃ§in veriyi hazÄ±rlayan Ã¶n iÅŸleme aÅŸamasÄ±dÄ±r. Veriyi yÃ¼klÃ¼yor, temizliyor, sayÄ±sal hale getiriyor ve makine Ã¶ÄŸrenmesi modellerine uygun ÅŸekilde bÃ¶lÃ¼yor.

Bag-of-Words (BoW) YaklaÅŸÄ±mÄ±

**Bag of Words (BoW) Nedir?**

Bag of Words, metin verilerini sayÄ±sal hale getirmek iÃ§in kullanÄ±lan en temel ve yaygÄ±n yÃ¶ntemlerden biridir. Temel mantÄ±ÄŸÄ± ÅŸudur:

Her belge (Ã¶rneÄŸin, bir film yorumu), iÃ§inde geÃ§en kelimelerin sÄ±klÄ±ÄŸÄ±yla temsil edilir.
Kelimelerin sÄ±rasÄ±, baÄŸlamÄ± veya dil bilgisel yapÄ±sÄ± Ã¶nemli deÄŸildir â€” yalnÄ±zca hangi kelime kaÃ§ kere geÃ§iyor, ona bakÄ±lÄ±r.


vectorizer = CountVectorizer(max_features=5000)
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

Bu satÄ±rlarda:

CountVectorizer, her bir metni (yorum) bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

Bu vektÃ¶r, eÄŸitim setinde en Ã§ok geÃ§en 5000 kelimeden hangileri yorumda var, varsa kaÃ§ kez var â€” bunu belirtir.

Yani her metin, sabit uzunlukta (5000 boyutlu) bir sayÄ±sal diziye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.

Ã–rnek:

Kelime	this	movie	was	amazing	terrible	...
Review 1	1	1	1	1	0	...
Review 2	1	1	1	0	1	...

ğŸ¤– Neden BoW KullanÄ±lÄ±r?
Basit ve hÄ±zlÄ±dÄ±r, Ã¶zellikle metin sÄ±nÄ±flandÄ±rmada (duygu analizi, spam tespiti vs.) etkili sonuÃ§lar verir.

Daha ileri yÃ¶ntemlere (TF-IDF, word2vec, BERT) geÃ§meden Ã¶nce BoW ile temel kavramlarÄ± anlamak Ã§ok faydalÄ±dÄ±r.

ğŸ§  Alternatifler?
EÄŸer metinlerde anlam, baÄŸlam, sÄ±ralama gibi kavramlar Ã¶nemliyse:

TfidfVectorizer (kelime aÄŸÄ±rlÄ±ÄŸÄ±na gÃ¶re vektÃ¶rleme)

word2vec, GloVe gibi gÃ¶mme (embedding) yÃ¶ntemleri

LSTM, Transformer, BERT gibi derin Ã¶ÄŸrenme temelli modeller kullanÄ±lÄ±r.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

import pandas as pd
import csv  # quoting iÃ§in

df = pd.read_csv("IMDB Dataset.csv", quoting=csv.QUOTE_NONE, on_bad_lines='skip', encoding='utf-8')

# Veriyi oku
df = pd.read_csv("IMDB Dataset.csv")

# NaN iÃ§eren satÄ±rlarÄ± temizle (Ã¶zellikle sentiment)
df = df.dropna(subset=['review', 'sentiment'])

# X ve y'yi ayÄ±r
X = df['review']
y = df['sentiment']

# EÄŸitim ve test verisine ayÄ±r
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# CountVectorizer ile metni sayÄ±sal verilere Ã§evir
vectorizer = CountVectorizer(max_features=5000)
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

# Naive Bayes modeli
nb = MultinomialNB()
nb.fit(X_train_counts, y_train)
y_pred = nb.predict(X_test_counts)

# SonuÃ§lar
print("CountVectorizer + Naive Bayes SonuÃ§larÄ±:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

"""TF-IDF YaklaÅŸÄ±mÄ±

**TF-IDF VektÃ¶rleÅŸtirme (Kelime AÄŸÄ±rlÄ±klÄ± VektÃ¶rleme)**

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

**Ne yapÄ±yor?**

TfidfVectorizer, metinlerde geÃ§en her kelimeye bir Ã¶nem skoru (aÄŸÄ±rlÄ±k) verir. Bu skor:

TF (Term Frequency) â†’ kelimenin bir belgede kaÃ§ kez geÃ§tiÄŸi.

IDF (Inverse Document Frequency) â†’ kelimenin tÃ¼m belgelerde ne kadar ayrÄ±ÅŸtÄ±rÄ±cÄ± olduÄŸuna bakar. Nadiren geÃ§en kelimeler daha yÃ¼ksek skor alÄ±r.

âœ… BÃ¶ylece, Ã§ok geÃ§en ama bilgi taÅŸÄ±mayan kelimeler ("the", "movie", "is" gibi) daha dÃ¼ÅŸÃ¼k aÄŸÄ±rlÄ±k alÄ±r.

2. Logistic Regression Modeliyle EÄŸitim

lr = LogisticRegression(max_iter=500)
lr.fit(X_train_tfidf, y_train)

LogisticRegression, TF-IDF ile sayÄ±sallaÅŸtÄ±rÄ±lmÄ±ÅŸ veriler Ã¼zerinde bir ikili sÄ±nÄ±flandÄ±rma (binary classification) modeli kurar.

max_iter=500, modelin eÄŸitim iÃ§in izin verilen maksimum iterasyon sayÄ±sÄ±dÄ±r. Ã‡ok veri olduÄŸunda bu deÄŸeri artÄ±rmak gerekebilir.

ğŸ” 3. Tahmin ve Performans Ã–lÃ§Ã¼mÃ¼

y_pred = lr.predict(X_test_tfidf)

print("\nTF-IDF + Logistic Regression SonuÃ§larÄ±:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

classification_report â†’ precision, recall, f1-score gibi detaylÄ± istatistikleri verir.

accuracy_score â†’ genel doÄŸruluÄŸu (doÄŸru tahmin sayÄ±sÄ± / toplam test sayÄ±sÄ±) hesaplar.

ğŸ¯ BoW ve TF-IDF FarkÄ±

Ã–zellik	Bag of Words (CountVectorizer)	TF-IDF
AÄŸÄ±rlÄ±klandÄ±rma	Kelime sÄ±klÄ±ÄŸÄ±na bakar	AÄŸÄ±rlÄ±klÄ± skorlama yapar
Ã‡ok geÃ§en kelimeler	Ã–nemli sayar	Az Ã¶nemli sayar
Bilgi yoÄŸunluÄŸu	Daha yÃ¼zeyseldir	Daha anlamlÄ±dÄ±r
"""

# TF-IDF vektÃ¶rleÅŸtirme
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Lojistik Regresyon modeli
lr = LogisticRegression(max_iter=500)
lr.fit(X_train_tfidf, y_train)
y_pred = lr.predict(X_test_tfidf)

print("\nTF-IDF + Logistic Regression SonuÃ§larÄ±:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

"""**Word Embedding Kullanarak Ã‡Ã¶zÃ¼m**

Tokenization ve Padding

**ğŸ”¢ 1. Tokenizer AyarlarÄ±**

max_words = 10000
max_len = 200

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

max_words=10000: En sÄ±k geÃ§en 10.000 kelime kullanÄ±lacak. Daha az geÃ§en kelimeler gÃ¶z ardÄ± edilir.

Tokenizer: Keras'tan gelen bu sÄ±nÄ±f, kelimeleri sayÄ±sal indekslere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

fit_on_texts(X_train): EÄŸitim verisinde geÃ§en kelimelere birer numara verir (Ã¶rn. "the" â†’ 1, "movie" â†’ 2 ... gibi).

**ğŸ”¢ 2. Metinleri SayÄ±sal Dizilere Ã‡evirme**

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

Her cÃ¼mle, iÃ§inde geÃ§en kelimelerin indeks listesi haline getirilir.

Ã–rnek:
"This movie is great!" â†’ [13, 25, 2, 487]

ğŸ“ 3. Padding (Dizileri EÅŸitleme)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)

pad_sequences: Her dizi 200 uzunluÄŸunda olacak ÅŸekilde kesilir veya sÄ±fÄ±rlarla doldurulur.

Bu, LSTM gibi modellerin sabit uzunlukta giriÅŸ almasÄ±nÄ± saÄŸlar.

Ã–rneÄŸin:
[13, 25, 2, 487] â†’ [0, 0, 0, ..., 13, 25, 2, 487] (200 eleman)
"""

# Tokenizer ayarlarÄ±
max_words = 10000
max_len = 200

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

# Metinleri sayÄ±sal dizilere Ã§evirme
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Padding uygulama
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)

"""Ã–ÄŸrenilmiÅŸ Embedding KatmanÄ± ile Model

**ğŸ”§ Model Mimarisi:**

ğŸ“Œ embedding_dim = 100

Bu, her kelimenin 100 boyutlu vektÃ¶r ile temsil edileceÄŸi anlamÄ±na gelir. Bu vektÃ¶rler model tarafÄ±ndan Ã¶ÄŸrenilir (veya Ã¶nceden eÄŸitilmiÅŸ vektÃ¶rler de kullanÄ±labilir).

**ğŸ§± Model KatmanlarÄ±:**

model = Sequential()

SÄ±ralÄ± bir model (katmanlar sÄ±rayla eklenir).

**1ï¸âƒ£ Embedding KatmanÄ±**

model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))

input_dim=max_words: SÃ¶zlÃ¼kteki toplam kelime sayÄ±sÄ± (10.000).

output_dim=100: Her kelime 100 boyutlu vektÃ¶re Ã§evrilecek.

input_length=200: Her giriÅŸ 200 kelimelik bir dizi.

ğŸ“Œ Bu katman, kelimeleri yoÄŸun (dense) vektÃ¶rler haline getirir. Ã–ÄŸrenilebilir parametrelerdir.

**2ï¸âƒ£ GlobalMaxPooling1D**

model.add(GlobalMaxPooling1D())

Her dizideki kelime vektÃ¶rleri iÃ§inden en yÃ¼ksek deÄŸeri alÄ±r.

LSTM yerine bu kullanÄ±lmÄ±ÅŸ, Ã§Ã¼nkÃ¼ daha hÄ±zlÄ± ve daha az hesaplama gerektirir.

3ï¸âƒ£ Dense + ReLU

model.add(Dense(64, activation='relu'))

64 nÃ¶ronlu tam baÄŸlantÄ±lÄ± katman.

ReLU aktivasyonu: negatifleri sÄ±fÄ±rlar, pozitifi geÃ§irir.

4ï¸âƒ£ Dropout

model.add(Dropout(0.5))

%50 oranÄ±nda nÃ¶ronlarÄ± rastgele pasifleÅŸtirir.

AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) azaltmak iÃ§in kullanÄ±lÄ±r.

5ï¸âƒ£ Ã‡Ä±kÄ±ÅŸ KatmanÄ±

model.add(Dense(1, activation='sigmoid'))

Tek nÃ¶ron: 0-1 arasÄ±nda bir Ã§Ä±ktÄ± Ã¼retir (pozitif veya negatif yorum).

sigmoid: OlasÄ±lÄ±k dÃ¶ndÃ¼rmek iÃ§in uygun.

âš™ï¸ Derleme AÅŸamasÄ±:

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

adam: Adaptif Ã¶ÄŸrenme oranÄ± kullanan verimli bir optimizasyon algoritmasÄ±.

binary_crossentropy: Ä°kili sÄ±nÄ±flama problemleri iÃ§in uygun kayÄ±p fonksiyonu.

accuracy: DoÄŸruluk metriÄŸi kullanÄ±lÄ±yor.

ğŸ§  EÄŸitim:

history = model.fit(
    X_train_pad, y_train_encoded,
    epochs=10,
    batch_size=128,
    validation_split=0.2,
    callbacks=[EarlyStopping(monitor='val_loss', patience=3)]
)


X_train_pad: SayÄ±sal hale getirilmiÅŸ ve pad edilmiÅŸ metin girdileri.

y_train_encoded: positive = 1, negative = 0 olarak kodlanmÄ±ÅŸ etiketler.

validation_split=0.2: %20â€™si validasyon iÃ§in ayrÄ±lÄ±r.

EarlyStopping: 3 epoch boyunca iyileÅŸme olmazsa durdurur (overfittingâ€™i engeller).

âœ… Bu modelin avantajlarÄ±:

EÄŸitim hÄ±zÄ± oldukÃ§a iyi (LSTM gibi zaman alan katmanlar yok).

Embedding + GlobalMaxPooling yapÄ±sÄ±, daha dÃ¼ÅŸÃ¼k hesaplama maliyetine sahip.

Dropout sayesinde genelleme kabiliyeti yÃ¼ksek.
"""

from sklearn.preprocessing import LabelEncoder

# Etiketleri sayÄ±ya Ã§evir
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

embedding_dim = 100

model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))
model.add(GlobalMaxPooling1D())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(X_train_pad, y_train_encoded,
                    epochs=10,
                    batch_size=128,
                    validation_split=0.2,
                    callbacks=[EarlyStopping(monitor='val_loss', patience=3)])

"""Ã–nceden EÄŸitilmiÅŸ Embedding (GloVe) KullanÄ±mÄ±

ğŸ”Œ 1. GloVe DosyalarÄ±nÄ±n Ä°ndirilmesi

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

ğŸ· 2. Etiketleri SayÄ±ya Ã‡evirme

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

positive, negative gibi metinsel etiketleri 0 ve 1 gibi sayÄ±lara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

ğŸ“š 3. GloVe VektÃ¶rlerini BelleÄŸe Alma

embeddings_index = {}
with open('glove.6B.100d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

GloVe dosyasÄ±nÄ± satÄ±r satÄ±r okur.

Her kelimenin karÅŸÄ±lÄ±k geldiÄŸi 100 boyutlu vektÃ¶rÃ¼ bir dict yapÄ±sÄ±na atar.

ğŸ§± 4. Embedding Matrisi OluÅŸturma

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in tokenizer.word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

Modelde kullanÄ±lacak ilk max_words kadar kelime iÃ§in GloVe vektÃ¶rleri atanÄ±r.

EÄŸer kelimenin GloVe karÅŸÄ±lÄ±ÄŸÄ± varsa, embedding matrisine eklenir.

ğŸ§  5. Model Mimarisi (GloVe + LSTM)

model_glove = Sequential()
model_glove.add(Embedding(..., weights=[embedding_matrix], trainable=False))

model_glove = Sequential()
model_glove.add(Embedding(..., weights=[embedding_matrix], trainable=False))


GloVe vektÃ¶rleri ile baÅŸlatÄ±lÄ±r.

trainable=False: GloVe aÄŸÄ±rlÄ±klarÄ± sabit kalÄ±r, Ã¶ÄŸrenilmez.

ğŸ”„ LSTM KatmanÄ±:

model_glove.add(LSTM(64, return_sequences=True))

Metinlerin sÄ±rayla iÅŸlenmesini saÄŸlar.

return_sequences=True: LSTM tÃ¼m zaman adÄ±mlarÄ±nÄ±n Ã§Ä±ktÄ±sÄ±nÄ± dÃ¶ndÃ¼rÃ¼r, Ã§Ã¼nkÃ¼ ardÄ±ndan GlobalMaxPooling1D var.

â• Devam KatmanlarÄ±:

model_glove.add(GlobalMaxPooling1D())
model_glove.add(Dense(64, activation='relu'))
model_glove.add(Dropout(0.5))
model_glove.add(Dense(1, activation='sigmoid'))


Ã–nceki Ã¶rneÄŸe benzer yapÄ±dÄ±r ama bu kez daha anlamlÄ± vektÃ¶rlerle Ã§alÄ±ÅŸÄ±r.

âš™ï¸ Derleme ve EÄŸitim:

model_glove.compile(...)
history = model.fit(...)


Model aynÄ± ÅŸekilde adam optimizer, binary_crossentropy loss ve accuracy metriÄŸi ile eÄŸitilir.

EarlyStopping: validasyon kaybÄ± geliÅŸmezse eÄŸitimi durdurur.

**ğŸ” SonuÃ§:**
Bu model, Ã¶nceki modele gÃ¶re daha semantik olarak zengin bir temele sahiptir. Ã‡Ã¼nkÃ¼:

Her kelime, sadece sÄ±klÄ±ÄŸa deÄŸil anlama gÃ¶re temsil edilir.

LSTM ile sÄ±radaki kelime baÄŸlamÄ± dikkate alÄ±nÄ±r.
"""

# GloVe embedding'leri yÃ¼kleme (Colab'de Ã§alÄ±ÅŸtÄ±rÄ±yorsanÄ±z Ã¶nce indirmelisiniz)
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip
from sklearn.preprocessing import LabelEncoder

# Etiketleri sayÄ±ya Ã§evir
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# GloVe embedding'lerini yÃ¼kleme
embeddings_index = {}
with open('glove.6B.100d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Embedding matrisini oluÅŸturma
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in tokenizer.word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

# GloVe embedding'li model
model_glove = Sequential()
model_glove.add(Embedding(input_dim=max_words,
                          output_dim=embedding_dim,
                          input_length=max_len,
                          weights=[embedding_matrix],
                          trainable=False))  # Ã–nceden eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klarÄ± dondurma
model_glove.add(LSTM(64, return_sequences=True))
model_glove.add(GlobalMaxPooling1D())
model_glove.add(Dense(64, activation='relu'))
model_glove.add(Dropout(0.5))
model_glove.add(Dense(1, activation='sigmoid'))

model_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(X_train_pad, y_train_encoded,
                    epochs=10,
                    batch_size=128,
                    validation_split=0.2,
                    callbacks=[EarlyStopping(monitor='val_loss', patience=3)])

"""SonuÃ§larÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±"""

import matplotlib.pyplot as plt
import seaborn as sns

# Seaborn stilini uygula
sns.set(style="whitegrid")

# SkorlarÄ± Ã¶nceden hesapla (Ã¶nceki aÃ§Ä±klamaya gÃ¶re)
dnn_score = model.evaluate(X_test_pad, y_test_encoded, verbose=0)[1]
glove_score = model_glove.evaluate(X_test_pad, y_test_encoded, verbose=0)[1]

# Model isimleri ve baÅŸarÄ±mlarÄ±
methods = ['BoW + Naive Bayes', 'TF-IDF + Logistic Reg.', 'Learned Embedding + DNN', 'GloVe Embedding + LSTM']
accuracies = [
    accuracy_score(y_test, nb.predict(X_test_counts)),
    accuracy_score(y_test, lr.predict(X_test_tfidf)),
    dnn_score,
    glove_score
]

# Grafik Ã§izimi
plt.figure(figsize=(12, 6))
bars = sns.barplot(x=methods, y=accuracies, palette='pastel')

# DoÄŸruluk deÄŸerlerini Ã§ubuklarÄ±n Ã¼zerine yaz
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.01, f"{v:.4f}", ha='center', va='bottom', fontsize=12, fontweight='bold')

# BaÅŸlÄ±k ve etiketler
plt.title('ğŸ“Š Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=16, fontweight='bold')
plt.ylabel('DoÄŸruluk (Accuracy)', fontsize=13)
plt.ylim(0.4, 1.0)
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

"""**KarmaÅŸÄ±klÄ±k Matrisi (Confusion Matrix):**

Her modelin performansÄ±nÄ± daha detaylÄ± bir ÅŸekilde gÃ¶rmek iÃ§in, doÄŸruluk oranlarÄ±nÄ±n yanÄ± sÄ±ra hangi sÄ±nÄ±flarÄ±n doÄŸru veya yanlÄ±ÅŸ tahmin edildiÄŸini gÃ¶steren karmaÅŸÄ±klÄ±k matrislerini hesaplayabiliriz. Bu, modelin hangi sÄ±nÄ±flarda gÃ¼Ã§lÃ¼ olduÄŸunu veya hangi sÄ±nÄ±flarda zayÄ±f kaldÄ±ÄŸÄ±nÄ± anlamamÄ±za yardÄ±mcÄ± olabilir.
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Naive Bayes
y_pred_nb = nb.predict(X_test_counts)
cm_nb = confusion_matrix(y_test, y_pred_nb)

# Logistic Regression
y_pred_lr = lr.predict(X_test_tfidf)
cm_lr = confusion_matrix(y_test, y_pred_lr)

# Ã–ÄŸrenilmiÅŸ Embedding + DNN
y_pred_dnn = model.predict(X_test_pad)
cm_dnn = confusion_matrix(y_test_encoded, (y_pred_dnn > 0.5).astype(int))

# GloVe Embedding + LSTM
y_pred_glove = model_glove.predict(X_test_pad)
cm_glove = confusion_matrix(y_test_encoded, (y_pred_glove > 0.5).astype(int))

# GÃ¶rselleÅŸtirme
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0], cbar=False)
axes[0, 0].set_title('Naive Bayes - Confusion Matrix')
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1], cbar=False)
axes[0, 1].set_title('Logistic Regression - Confusion Matrix')
sns.heatmap(cm_dnn, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0], cbar=False)
axes[1, 0].set_title('DNN - Confusion Matrix')
sns.heatmap(cm_glove, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1], cbar=False)
axes[1, 1].set_title('GloVe + LSTM - Confusion Matrix')

plt.tight_layout()
plt.show()

"""**Genel Yorum:**

Bu dÃ¶rt modelin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±, doÄŸal dil iÅŸleme ve metin sÄ±nÄ±flandÄ±rma gÃ¶revlerinde her bir modelin ne kadar etkili olduÄŸunu gÃ¶rsel olarak gÃ¶steriyor. Her bir karÄ±ÅŸÄ±klÄ±k matrisi, modelin performansÄ±nÄ± analiz etmek iÃ§in Ã§ok faydalÄ±dÄ±r.

DNN ve GloVe + LSTM gibi derin Ã¶ÄŸrenme modelleri, genellikle Naive Bayes ve Lojistik Regresyon'dan daha baÅŸarÄ±lÄ± olabilir, Ã§Ã¼nkÃ¼ bu modeller daha karmaÅŸÄ±k iliÅŸkileri Ã¶ÄŸrenme kapasitesine sahiptir.

Ancak, her modelin gÃ¼Ã§lÃ¼ ve zayÄ±f yÃ¶nleri olduÄŸu iÃ§in, kullanÄ±cÄ±nÄ±n veri setine ve uygulama amacÄ±na gÃ¶re hangi modelin en iyi sonucu vereceÄŸi belirlenebilir.

SonuÃ§:
Bu gÃ¶rselleÅŸtirmeler, dÃ¶rt farklÄ± modelin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± iÃ§in oldukÃ§a faydalÄ±dÄ±r. KarÄ±ÅŸÄ±klÄ±k matrisleri, modelin genel baÅŸarÄ±sÄ±nÄ±, yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rma oranlarÄ±nÄ± ve doÄŸru sÄ±nÄ±flandÄ±rmalarÄ±n miktarÄ±nÄ± gÃ¶stererek, hangi modelin daha uygun olduÄŸuna karar verilmesine yardÄ±mcÄ± olur.

**F1 Skoru, Precision ve Recall:**

DoÄŸruluk (accuracy) dÄ±ÅŸÄ±nda, modellerin baÅŸarÄ±larÄ±nÄ± daha iyi anlayabilmek iÃ§in precision, recall, ve F1 score gibi metrikleri de gÃ¶z Ã¶nÃ¼nde bulundurabiliriz. Bu metrikler Ã¶zellikle dengesiz veri setlerinde Ã¶nemlidir.
"""

from sklearn.metrics import classification_report

# Naive Bayes
print("Naive Bayes Classification Report:")
print(classification_report(y_test, y_pred_nb))

# Logistic Regression
print("Logistic Regression Classification Report:")
print(classification_report(y_test, y_pred_lr))

# DNN
y_pred_dnn = (y_pred_dnn > 0.5).astype(int)
print("DNN Classification Report:")
print(classification_report(y_test_encoded, y_pred_dnn))

# GloVe LSTM
y_pred_glove = (y_pred_glove > 0.5).astype(int)
print("GloVe + LSTM Classification Report:")
print(classification_report(y_test_encoded, y_pred_glove))

"""**ROC ve AUC EÄŸrisi:**

Modelin karar sÄ±nÄ±rlarÄ±nÄ± daha iyi anlamak iÃ§in ROC (Receiver Operating Characteristic) eÄŸrisini ve AUC (Area Under Curve) skoru hesaplayabiliriz. Bu Ã¶zellikle modelin tahmin ettiÄŸi olasÄ±lÄ±klarÄ± incelemek iÃ§in faydalÄ±dÄ±r.

**Bu kodumuz: **

Receiver Operating Characteristic (ROC) eÄŸrisini ve AUC (Area Under the Curve) deÄŸerlerini hesaplar ve gÃ¶rselleÅŸtirir. ROC eÄŸrisi, sÄ±nÄ±flandÄ±rma modelinin baÅŸarÄ±sÄ±nÄ± deÄŸerlendirmek iÃ§in yaygÄ±n bir yÃ¶ntemdir ve genellikle modelin True Positive Rate (TPR) ile False Positive Rate (FPR)'Ä± arasÄ±ndaki iliÅŸkiyi gÃ¶rsel olarak sunar. AUC deÄŸeri ise ROC eÄŸrisinin altÄ±nda kalan alanÄ± temsil eder ve modelin genel baÅŸarÄ±sÄ±nÄ± sayÄ±sal olarak Ã¶zetler.
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Etiketleri sayÄ±sal deÄŸerlere dÃ¶nÃ¼ÅŸtÃ¼r
label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)

# Naive Bayes
fpr_nb, tpr_nb, _ = roc_curve(y_test_encoded, nb.predict_proba(X_test_counts)[:, 1])
auc_nb = auc(fpr_nb, tpr_nb)

# Logistic Regression
fpr_lr, tpr_lr, _ = roc_curve(y_test_encoded, lr.predict_proba(X_test_tfidf)[:, 1])
auc_lr = auc(fpr_lr, tpr_lr)

# DNN
fpr_dnn, tpr_dnn, _ = roc_curve(y_test_encoded, y_pred_dnn)
auc_dnn = auc(fpr_dnn, tpr_dnn)

# GloVe LSTM
fpr_glove, tpr_glove, _ = roc_curve(y_test_encoded, y_pred_glove)
auc_glove = auc(fpr_glove, tpr_glove)

# ROC EÄŸrisi GÃ¶rselleÅŸtirme
plt.figure(figsize=(10, 6))
plt.plot(fpr_nb, tpr_nb, color='blue', lw=2, label='Naive Bayes (AUC = %0.2f)' % auc_nb)
plt.plot(fpr_lr, tpr_lr, color='orange', lw=2, label='Logistic Regression (AUC = %0.2f)' % auc_lr)
plt.plot(fpr_dnn, tpr_dnn, color='green', lw=2, label='DNN (AUC = %0.2f)' % auc_dnn)
plt.plot(fpr_glove, tpr_glove, color='red', lw=2, label='GloVe + LSTM (AUC = %0.2f)' % auc_glove)

plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""**Kodun Ä°ÅŸleyiÅŸi:**

1)Etiketlerin SayÄ±sal DeÄŸerlere DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi:


LabelEncoder kullanÄ±larak, test verisi Ã¼zerindeki etiketler (y_test) sayÄ±sal deÄŸerlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r. Bu adÄ±m, ROC eÄŸrisinin hesaplanabilmesi iÃ§in gereklidir.

2)ROC EÄŸrisinin HesaplanmasÄ±:

Her model iÃ§in True Positive Rate (TPR) ve False Positive Rate (FPR) hesaplanÄ±r. Bunun iÃ§in her modelin predict_proba fonksiyonu kullanÄ±larak probabilistik tahminler alÄ±nÄ±r (yani, her sÄ±nÄ±f iÃ§in olasÄ±lÄ±k tahminleri).

Bu olasÄ±lÄ±klar, gerÃ§ek etiketlerle karÅŸÄ±laÅŸtÄ±rÄ±larak ROC eÄŸrisinin iki bileÅŸeni olan FPR ve TPR hesaplanÄ±r.

AUC deÄŸeri, ROC eÄŸrisinin altÄ±nda kalan alanÄ± hesaplar. AUC, modelin doÄŸru sÄ±nÄ±flandÄ±rma yeteneÄŸini bir sayÄ±sal deÄŸerle Ã¶zetler.

3)ROC EÄŸrisinin GÃ¶rselleÅŸtirilmesi:

matplotlib.pyplot kullanÄ±larak her model iÃ§in ROC eÄŸrisi Ã§izilir.

Her modelin ROC eÄŸrisinin yanÄ±na, o modele ait AUC deÄŸeri eklenir.

Gray Ã§izgi (kesik Ã§izgi), rasgele tahmin yapan bir modelin ROC eÄŸrisini temsil eder (AUC = 0.5).

EÄŸriler, False Positive Rate (x-ekseni) ile True Positive Rate (y-ekseni) arasÄ±ndaki iliÅŸkiyi gÃ¶rselleÅŸtirir.

**KapsayÄ±cÄ± Performans DeÄŸerlendirme:**

Modelin farklÄ± veri kÃ¼mesindeki performansÄ±nÄ± gÃ¶rmek iÃ§in her bir modelin eÄŸitim ve test sonuÃ§larÄ± arasÄ±ndaki farkÄ± analiz edebiliriz. AyrÄ±ca, overfitting durumunun olup olmadÄ±ÄŸÄ±nÄ± kontrol edebiliriz.
"""

print("\nModel Overfitting KontrolÃ¼:")
print(f"Naive Bayes EÄŸitim DoÄŸruluÄŸu: {accuracy_score(y_train, nb.predict(X_train_counts))}")
print(f"Logistic Regression EÄŸitim DoÄŸruluÄŸu: {accuracy_score(y_train, lr.predict(X_train_tfidf))}")
print(f"DNN EÄŸitim DoÄŸruluÄŸu: {model.evaluate(X_train_pad, y_train_encoded)[1]}")
print(f"GloVe + LSTM EÄŸitim DoÄŸruluÄŸu: {model_glove.evaluate(X_train_pad, y_train_encoded)[1]}")

"""**Confusion Matrix (KarÄ±ÅŸÄ±klÄ±k Matrisi):**

Confusion matrix, modelin doÄŸru ve yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalarÄ±nÄ± gÃ¶rselleÅŸtirmenizi saÄŸlar. Bu, modelin baÅŸarÄ±sÄ±z olduÄŸu yerleri anlamanÄ±za yardÄ±mcÄ± olur.
"""

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)  # y_test'i sayÄ±sala dÃ¶nÃ¼ÅŸtÃ¼r
y_pred_numeric = nb.predict(X_test_counts)  # model tahminlerini al

# Model tahminlerini sayÄ±sala dÃ¶nÃ¼ÅŸtÃ¼rme
y_pred_encoded = label_encoder.transform(y_pred_numeric)

# KarÄ±ÅŸÄ±klÄ±k Matrisi
cm = confusion_matrix(y_test_encoded, y_pred_encoded)

# KarÄ±ÅŸÄ±klÄ±k Matrisi GÃ¶rselleÅŸtirme
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Tahmin Edilen')
plt.ylabel('GerÃ§ek Etiket')
plt.title('Naive Bayes Modeli - KarÄ±ÅŸÄ±klÄ±k Matrisi')
plt.show()

"""**Precision-Recall Curve:**

Precision-Recall eÄŸrisini de ROC eÄŸrisinin yanÄ± sÄ±ra gÃ¶rselleÅŸtirebiliriz. Bu eÄŸri, sÄ±nÄ±flar arasÄ±nda dengesizlik olduÄŸunda daha anlamlÄ± olabilir.
"""

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import auc

# Precision-Recall eÄŸrisini Ã§izme
precision, recall, _ = precision_recall_curve(y_test_encoded, nb.predict_proba(X_test_counts)[:, 1])
pr_auc = auc(recall, precision)

plt.figure(figsize=(10, 6))
plt.plot(recall, precision, color='blue', lw=2, label='Naive Bayes (AUC = %0.2f)' % pr_auc)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

"""**K-fold Cross Validation:**

Modeli daha gÃ¼venilir bir ÅŸekilde deÄŸerlendirmek iÃ§in k-fold Ã§apraz doÄŸrulama yapabiliriz. Bu, modelimizin  genelleme yeteneÄŸini daha doÄŸru bir ÅŸekilde Ã¶lÃ§memizi saÄŸlar.
"""

from sklearn.model_selection import cross_val_score

# 5-fold Ã§apraz doÄŸrulama
cv_scores = cross_val_score(nb, X_train_counts, y_train_encoded, cv=5)
print("Ã‡apraz DoÄŸrulama SonuÃ§larÄ±:", cv_scores)
print("Ortalama DoÄŸruluk:", np.mean(cv_scores))

"""**Learning Curve:**

Modelin Ã¶ÄŸrenme sÃ¼recini gÃ¶rselleÅŸtirmek iÃ§in learning curve (Ã¶ÄŸrenme eÄŸrisi) Ã§izilebilir. Bu, eÄŸitim verisi arttÄ±kÃ§a modelin doÄŸruluÄŸunun nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶sterir.

Bu Kodumuzla:

Naive Bayes sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ±n Ã¶ÄŸrenme eÄŸrisini Ã§izmek iÃ§in kullandÄ±k. Ã–ÄŸrenme eÄŸrisi (learning curve), modelin eÄŸitim sÃ¼resiyle birlikte Ã¶ÄŸrenme sÃ¼recinin nasÄ±l ilerlediÄŸini ve modelin genel doÄŸruluÄŸunu nasÄ±l geliÅŸtirdiÄŸini gÃ¶steren bir grafik tÃ¼rÃ¼nÃ¼ gÃ¶rselleÅŸtirmek istedik.

Kodun Ä°ÅŸleyiÅŸi:
learning_curve Fonksiyonu:

learning_curve fonksiyonu, verilen model (burada Naive Bayes) iÃ§in Ã¶ÄŸrenme eÄŸrisini hesaplamak Ã¼zere kullanÄ±lÄ±r. Bu fonksiyon, eÄŸitim verilerinin farklÄ± alt kÃ¼me boyutlarÄ±nda modelin doÄŸruluÄŸunu deÄŸerlendirir.

train_sizes deÄŸiÅŸkeni, modelin eÄŸitiminde kullanÄ±lan farklÄ± eÄŸitim veri boyutlarÄ±nÄ± belirtir.

train_scores ve test_scores, her eÄŸitim boyutu iÃ§in modelin eÄŸitim verisi Ã¼zerindeki doÄŸruluÄŸu ve test verisi Ã¼zerindeki doÄŸruluÄŸu (modelin genelleme kapasitesi) hakkÄ±nda bilgi saÄŸlar.

EÄŸitim ve Test SkorlarÄ±nÄ±n HesaplanmasÄ±:

EÄŸitim skoru (train_scores), modelin eÄŸitim verisi Ã¼zerinde gÃ¶sterdiÄŸi performansÄ± belirtir.

Test skoru (test_scores), modelin test verisi Ã¼zerindeki doÄŸruluÄŸunu gÃ¶sterir ve modelin genelleme yeteneÄŸi hakkÄ±nda bilgi verir.

np.mean() fonksiyonu, her eÄŸitim veri boyutu iÃ§in orta deÄŸeryi alÄ±r, bÃ¶ylece Ã§apraz doÄŸrulama (cross-validation) sonucu ortaya Ã§Ä±kan varyasyonu azaltÄ±r.

GrafiÄŸin Ã‡izilmesi:

train_sizes (eÄŸitim veri boyutlarÄ±) ile eÄŸitim ve test skorlarÄ±nÄ±n deÄŸiÅŸimini gÃ¶rselleÅŸtiren bir grafik oluÅŸturulur.

X-ekseni: EÄŸitim veri boyutlarÄ± (kaÃ§ Ã¶rnekle model eÄŸitildiÄŸi).

Y-ekseni: Modelin doÄŸruluÄŸu (accuracy).

EÄŸitim Skoru (Mavi Ã‡izgi): Modelin eÄŸitim verisi Ã¼zerindeki baÅŸarÄ±sÄ±nÄ± gÃ¶sterir.

Test Skoru (Turuncu Ã‡izgi): Modelin test verisi Ã¼zerindeki baÅŸarÄ±sÄ±nÄ± gÃ¶sterir.
"""

from sklearn.model_selection import learning_curve

# Ã–ÄŸrenme EÄŸrisini Ã‡izme
train_sizes, train_scores, test_scores = learning_curve(nb, X_train_counts, y_train_encoded, cv=5)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, np.mean(train_scores, axis=1), label="EÄŸitim Skoru", color='blue')
plt.plot(train_sizes, np.mean(test_scores, axis=1), label="Test Skoru", color='orange')
plt.xlabel('EÄŸitim Verisi SayÄ±sÄ±')
plt.ylabel('DoÄŸruluk (Accuracy)')
plt.title('Learning Curve - Naive Bayes')
plt.legend()
plt.show()

"""**GrafiÄŸin YorumlanmasÄ±:**

EÄŸitim Skoru (Mavi Ã‡izgi):

EÄŸitim verisi Ã¼zerinde doÄŸruluk, genellikle daha fazla eÄŸitim verisi ile artar.

EÄŸer eÄŸitim skoru Ã§ok yÃ¼ksek ve test skoru Ã§ok dÃ¼ÅŸÃ¼kse, modelin overfitting (aÅŸÄ±rÄ± uyum) yaptÄ±ÄŸÄ± dÃ¼ÅŸÃ¼nÃ¼lebilir.

Test Skoru (Turuncu Ã‡izgi):

Test skoru, modelin genelleme yeteneÄŸini gÃ¶sterir.

BaÅŸlangÄ±Ã§ta test skoru, eÄŸitim verisinden daha dÃ¼ÅŸÃ¼k olabilir, ancak daha fazla eÄŸitim verisi kullanÄ±ldÄ±kÃ§a test skoru da artar.

Test skoru, modelin ne kadar iyi genelleme yaptÄ±ÄŸÄ± hakkÄ±nda fikir verir. EÄŸer test skoru eÄŸitim skoruna yakÄ±nsa, modelin genelleme yeteneÄŸi iyidir.

**AUC Skorunun HesaplanmasÄ± ve KarÅŸÄ±laÅŸtÄ±rma:**

AUC skorlarÄ± Ã¼zerinden modellerin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± da faydalÄ± olabilir.
"""

# AUC hesaplama
auc_nb = auc(fpr_nb, tpr_nb)
auc_lr = auc(fpr_lr, tpr_lr)
auc_dnn = auc(fpr_dnn, tpr_dnn)
auc_glove = auc(fpr_glove, tpr_glove)

print(f"AUC Naive Bayes: {auc_nb:.4f}")
print(f"AUC Logistic Regression: {auc_lr:.4f}")
print(f"AUC DNN: {auc_dnn:.4f}")
print(f"AUC GloVe + LSTM: {auc_glove:.4f}")

"""**SonuÃ§lar ve Analiz**,

Bu Ã§alÄ±ÅŸmada dÃ¶rt farklÄ± yaklaÅŸÄ±mÄ± karÅŸÄ±laÅŸtÄ±rdÄ±k:

Bag-of-Words + Naive Bayes: En basit yaklaÅŸÄ±m olmasÄ±na raÄŸmen %82-83 doÄŸruluk saÄŸladÄ±.

TF-IDF + Logistic Regression: TF-IDF'in daha iyi Ã¶zellik Ã§Ä±karÄ±mÄ± sayesinde %87-88 doÄŸruluk elde edildi.

Ã–ÄŸrenilmiÅŸ Embedding + DNN: Neural network ve embedding katmanÄ± ile %85-86 doÄŸruluk.

GloVe Embedding + LSTM: Ã–nceden eÄŸitilmiÅŸ embedding'ler ve LSTM ile en iyi sonuÃ§ (%88-89).

Ã‡Ä±karÄ±mlar:

Word embedding yÃ¶ntemleri, Ã¶zellikle Ã¶nceden eÄŸitilmiÅŸ modeller (GloVe) kullanÄ±ldÄ±ÄŸÄ±nda en iyi performansÄ± verdi.

Geleneksel yÃ¶ntemler (Ã¶zellikle TF-IDF) basit olmalarÄ±na raÄŸmen oldukÃ§a iyi sonuÃ§lar Ã¼retebiliyor.

Neural network modelleri daha fazla veri ve hesaplama gÃ¼cÃ¼ gerektirirken, daha karmaÅŸÄ±k iliÅŸkileri Ã¶ÄŸrenebiliyor.

Ã–nceden eÄŸitilmiÅŸ embedding'ler, Ã¶zellikle kÃ¼Ã§Ã¼k veri setlerinde bÃ¼yÃ¼k avantaj saÄŸlÄ±yor.

**Bu Ã§alÄ±ÅŸma, word embedding yÃ¶ntemlerinin NLP problemlerindeki Ã¶nemini ve avantajlarÄ±nÄ± gÃ¶stermektedir.**

---


**projector.tensorflow.org GÃ¶rselleÅŸtirmesi**

GloVe modelinden alÄ±nan Ã¶nceden eÄŸitilmiÅŸ kelime vektÃ¶rlerini kullanarak, ilk 500 kelimeyi ve onlarÄ±n embedding vektÃ¶rlerini iÅŸleyip iki dosya oluÅŸturuyoruz: tensor.tsv ve metadata.tsv

**NOT:** tensorflow.orgda bu iki dosyayÄ± aynÄ± anda yÃ¼kleyemediÄŸimiz iÃ§in Ã¶zel olarak projector_data.tsv dosyasÄ±nÄ± oluÅŸtuduk(metadata.tsv ve tensor.tsv nin birleÅŸtirilmiÅŸ hali )
"""

import numpy as np

num_words = 500  # Ä°stenilen kelime sayÄ±sÄ±
glove_path = "glove.6B.100d.txt"

words = []
vectors = []

with open(glove_path, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= num_words:
            break
        values = line.strip().split()
        if len(values) != 101:  # 1 kelime + 100 boyut
            continue  # Eksik vektÃ¶rleri atla
        word = values[0]
        try:
            vector = [float(x) for x in values[1:]]  # SayÄ±sal dÃ¶nÃ¼ÅŸÃ¼m
            words.append(word)
            vectors.append(vector)
        except ValueError:
            continue  # GeÃ§ersiz sayÄ±sal deÄŸerleri atla

# tensor.tsv dosyasÄ±na yazma (boyut kontrolÃ¼ ile)
with open("tensor.tsv", "w", encoding='utf-8') as f:
    for vec in vectors:
        if len(vec) == 100:  # 100 boyutlu olduÄŸundan emin ol
            f.write("\t".join(map(str, vec)) + "\n")  # SayÄ±larÄ± stringe Ã§evir

# metadata.tsv dosyasÄ±na yazma
with open("metadata.tsv", "w", encoding='utf-8') as f:
    for word in words:
        f.write(word + "\n")

print(f"Ä°ÅŸlem tamamlandÄ±. {len(words)} kelime iÅŸlendi.")

"""GloVe Modeliyle Ã‡alÄ±ÅŸmak:

Kod, GloVe modelinin 100 boyutlu vektÃ¶rlerini iÃ§eren glove.6B.100d.txt dosyasÄ±nÄ± okur. Bu dosya, her satÄ±rda bir kelime ve o kelimeye ait 100 boyutlu vektÃ¶r iÃ§erir.

Kod, bu dosyadaki yalnÄ±zca ilk 500 kelimeyi seÃ§er. Bu, daha bÃ¼yÃ¼k dosyalarla Ã§alÄ±ÅŸÄ±rken veri boyutunu kÃ¼Ã§Ã¼ltmek ve iÅŸlem sÃ¼resini azaltmak iÃ§in kullanÄ±ÅŸlÄ±dÄ±r.

Verilerin Temizlenmesi ve Ä°ÅŸlenmesi:

Kod, her satÄ±rÄ± okuduktan sonra kelimeyi ve o kelimenin 100 boyutlu vektÃ¶rÃ¼nÃ¼ ayÄ±rÄ±r.

Eksik veya hatalÄ± vektÃ¶rler (veya geÃ§ersiz sayÄ±sal veriler) atlanarak yalnÄ±zca geÃ§erli veriler iÅŸlenir. Bu, verinin doÄŸruluÄŸunu saÄŸlamak adÄ±na Ã¶nemli bir adÄ±mdÄ±r.

Verilerin YazÄ±lmasÄ±:

tensor.tsv: Bu dosya, her bir kelimenin 100 boyutlu embedding vektÃ¶rlerini iÃ§erir ve her vektÃ¶r bir satÄ±rda, tab ile ayrÄ±lmÄ±ÅŸ sayÄ±lar olarak yazÄ±lÄ±r.

metadata.tsv: Bu dosya ise kelimelerin kendilerini iÃ§erir ve her kelime bir satÄ±ra yazÄ±lÄ±r. Bu dosya, kelimeler ve onlarÄ±n vektÃ¶rleri arasÄ±ndaki iliÅŸkiyi tutar.

Veri YazÄ±m FormatÄ±:

Tab ile ayrÄ±lmÄ±ÅŸ (TSV) formatÄ±, vektÃ¶rlerin ve kelimelerin dÃ¼zenli bir ÅŸekilde yazÄ±lmasÄ±nÄ± saÄŸlar. Bu format, Ã¶zellikle gÃ¶rselleÅŸtirme araÃ§larÄ± (Ã¶rneÄŸin TensorFlow Projector) tarafÄ±ndan kabul edilen yaygÄ±n bir formattÄ±r.
"""

# Tokenizer'dan ilk max_words kadar kelimeyi al
word_list = list(tokenizer.word_index.keys())[:max_words]

import numpy as np
np.savetxt('tensor.tsv', embedding_matrix[:len(word_list)], delimiter='\t')

word_list = list(tokenizer.word_index.keys())[:len(embedding_matrix)]
with open('metadata.tsv', 'w', encoding='utf-8') as f:
    for word in word_list:
        f.write(f"{word}\n")

print(embedding_matrix.shape)
print(len(word_list))

# Sadece embedding_matrix iÃ§inde bulunan kelimeleri seÃ§ (i < max_words)
word_list = []
valid_embeddings = []

for word, i in tokenizer.word_index.items():
    if i < max_words:
        vec = embedding_matrix[i]
        if vec is not None and np.any(vec):  # SÄ±fÄ±r olmayan vektÃ¶r
            word_list.append(word)
            valid_embeddings.append(vec)

valid_embeddings = np.array(valid_embeddings)

# tensor.tsv
np.savetxt('tensor.tsv', valid_embeddings, delimiter='\t')

# metadata.tsv
with open('metadata.tsv', 'w', encoding='utf-8') as f:
    for word in word_list:
        f.write(f"{word}\n")

import zipfile
with zipfile.ZipFile('imdb_embeddings.zip', 'w') as zipf:
    zipf.write('tensor.tsv')
    zipf.write('metadata.tsv')

# Tek bir .tsv dosyasÄ±na vektÃ¶r + metadata yazma (Projector Ã¶zel formatÄ±)
with open('projector_data.tsv', 'w', encoding='utf-8') as f:
    for word, vec in zip(words, vectors):
        f.write(word + "\t" + "\t".join(map(str, vec)) + "\n")

"""Genel DeÄŸerlendirme:

Verimli Veri Ä°ÅŸleme: Kod, veriyi okurken yalnÄ±zca ilk 500 kelimeyi alarak iÅŸlem sÃ¼resini kÄ±saltÄ±r. Bu, bÃ¼yÃ¼k veri setleri ile Ã§alÄ±ÅŸÄ±rken Ã¶nemli bir iyileÅŸtirmedir.

Temizleme ve Hata YÃ¶netimi: Eksik veya hatalÄ± vektÃ¶rler ve sayÄ±sal deÄŸerler dÃ¼zgÃ¼n bir ÅŸekilde atlanÄ±r, bu da verinin kalitesini ve doÄŸruluÄŸunu artÄ±rÄ±r.

GÃ¶rselleÅŸtirme ve Analiz: tensor.tsv ve metadata.tsv dosyalarÄ±, kelimelerin vektÃ¶rlerini gÃ¶rselleÅŸtirme ve kelimeler arasÄ±ndaki benzerlikleri analiz etme aÃ§Ä±sÄ±ndan kullanÄ±labilir. Ã–zellikle, TensorFlow Projector gibi araÃ§larla gÃ¶rselleÅŸtirme yaparak kelimeler arasÄ±ndaki iliÅŸkileri keÅŸfetmek mÃ¼mkÃ¼ndÃ¼r.

**AÅŸaÄŸÄ±daki Ã¶rnekler raporumuzda belirttiÄŸimiz Ã¶rneklerin isimlerini bulmak iÃ§in yazdÄ±ÄŸÄ±mÄ±z kodlardÄ±r.**

1. Ã–rnek : Through
"""

print("This word is main word : " + word_list[137])
print(word_list[420])
print(word_list[138])
print(word_list[66])

"""2. Ã–rnek : Mean kelimesi"""

print("This word is main word : " + word_list[366])
print(word_list[320])
print(word_list[294])
print(word_list[145])

"""3. Ã–rnek: Little kelimesi"""

print( "This word is main word : " + word_list[116] )
print(word_list[433])
print(word_list[40])
print(word_list[159])

"""

---

"""